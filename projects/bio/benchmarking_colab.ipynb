{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Benchmarking\n",
        "This notebook benchmarks published genomic language models (DNABERT, HyenaDNA, Nucleotide Transformer) on Genomic Benchmarks.\n",
        "\n",
        "**Requirements:**\n",
        "- Google Colab with GPU runtime\n",
        "- Google Cloud Storage bucket with Genomic Benchmarks datasets\n",
        "- GCS authentication"
      ],
      "metadata": {
        "id": "h5NWa3WbxdD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QXxj6jB1xgvx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkeUVAzfz-5j"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch pandas scikit-learn google-cloud-storage tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRqvurj30cDn"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 2: Setup Google Cloud authentication\n",
        "# ============================================\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVEipLC_0cYb"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 3: Import all libraries\n",
        "# ============================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score, confusion_matrix\n",
        "from google.cloud import storage\n",
        "import io\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CUkdAA90chy"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 4: Define dataset class for DNA sequences\n",
        "# ============================================\n",
        "class DNADataset(Dataset):\n",
        "    \"\"\"Dataset class for DNA sequences\"\"\"\n",
        "    def __init__(self, sequences, labels=None):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.labels is not None:\n",
        "            return self.sequences[idx], self.labels[idx]\n",
        "        return self.sequences[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXMlih4E0cmt"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 5: Helper functions for GCS\n",
        "# ============================================\n",
        "def load_csv_from_gcs(bucket_name, file_path):\n",
        "    \"\"\"Load CSV from Google Cloud Storage\"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_path)\n",
        "    data = blob.download_as_string()\n",
        "    data_file = io.StringIO(data.decode(\"utf-8\"))\n",
        "    df = pd.read_csv(data_file)\n",
        "    return df\n",
        "\n",
        "def save_results_to_gcs(bucket_name, file_path, results_dict):\n",
        "    \"\"\"Save results dictionary as JSON to GCS\"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_path)\n",
        "    blob.upload_from_string(json.dumps(results_dict, indent=2))\n",
        "    print(f\"Results saved to gs://{bucket_name}/{file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpCqWohN0crZ"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 6: DNABERT specific functions\n",
        "# ============================================\n",
        "def prepare_dnabert_input(sequence, k=6):\n",
        "    \"\"\"\n",
        "    Convert DNA sequence to k-mer representation for DNABERT\n",
        "    DNABERT uses k-mer tokenization with k=6 by default\n",
        "    \"\"\"\n",
        "    # Convert to uppercase\n",
        "    sequence = sequence.upper()\n",
        "    # Create k-mers with spaces between them\n",
        "    kmers = []\n",
        "    for i in range(len(sequence) - k + 1):\n",
        "        kmers.append(sequence[i:i+k])\n",
        "    return ' '.join(kmers)\n",
        "\n",
        "def benchmark_dnabert(train_df, test_df, dataset_name, bucket_name, max_length=512):\n",
        "    \"\"\"\n",
        "    Benchmark DNABERT on a genomic dataset\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Benchmarking DNABERT on {dataset_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Load DNABERT model and tokenizer\n",
        "    print(\"Loading DNABERT model...\")\n",
        "    model_name = \"zhihan1996/DNA_bert_6\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Determine number of classes from the data\n",
        "    num_classes = len(train_df['label'].unique())\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "    # Load model with correct number of output classes\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_classes,\n",
        "        trust_remote_code=True,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Prepare data\n",
        "    print(\"Preparing sequences for DNABERT (k-mer tokenization)...\")\n",
        "    train_sequences = [prepare_dnabert_input(seq) for seq in tqdm(train_df['sequence'].values, desc=\"Processing train\")]\n",
        "    test_sequences = [prepare_dnabert_input(seq) for seq in tqdm(test_df['sequence'].values, desc=\"Processing test\")]\n",
        "\n",
        "    train_labels = train_df['label'].values\n",
        "    test_labels = test_df['label'].values\n",
        "\n",
        "    # Fine-tuning DNABERT\n",
        "    print(\"\\nFine-tuning DNABERT...\")\n",
        "    model.train()\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
        "\n",
        "    # Training parameters\n",
        "    batch_size = 16\n",
        "    num_epochs = 1\n",
        "\n",
        "    # Create dataset\n",
        "    train_dataset = DNADataset(train_sequences, train_labels)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch_sequences, batch_labels in progress_bar:\n",
        "            # Tokenize batch\n",
        "            inputs = tokenizer(\n",
        "                batch_sequences,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "\n",
        "            labels = torch.tensor(batch_labels).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(**inputs, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1} - Average loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    test_dataset = DNADataset(test_sequences, test_labels)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_sequences, batch_labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            inputs = tokenizer(\n",
        "                batch_sequences,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(batch_labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    mcc = matthews_corrcoef(all_labels, all_predictions)\n",
        "    f1 = f1_score(all_labels, all_predictions, average='weighted' if num_classes > 2 else 'binary')\n",
        "\n",
        "    print(f\"\\nResults for {dataset_name}:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"MCC: {mcc:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(all_labels, all_predictions))\n",
        "\n",
        "    # Save results\n",
        "    results = {\n",
        "        'dataset': dataset_name,\n",
        "        'model': 'DNABERT',\n",
        "        'accuracy': float(accuracy),\n",
        "        'mcc': float(mcc),\n",
        "        'f1': float(f1),\n",
        "        'num_train_samples': len(train_df),\n",
        "        'num_test_samples': len(test_df),\n",
        "        'num_classes': num_classes\n",
        "    }\n",
        "\n",
        "    # Save to GCS\n",
        "    results_path = f\"benchmark_results/dnabert/{dataset_name}_results.json\"\n",
        "    save_results_to_gcs(bucket_name, results_path, results)\n",
        "\n",
        "    # Clear GPU memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcuCTZYr0cvD"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 7: HyenaDNA benchmark function (COMPLETE FIXED VERSION)\n",
        "# ============================================\n",
        "def benchmark_hyena_dna(train_df, test_df, dataset_name, bucket_name, max_length=1024):\n",
        "    \"\"\"\n",
        "    Benchmark HyenaDNA on a genomic dataset\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Benchmarking HyenaDNA on {dataset_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    print(\"Loading HyenaDNA model...\")\n",
        "\n",
        "    try:\n",
        "        from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "        model_name = \"kuleshov-group/hyenadna-small-32k-seqlen\"\n",
        "\n",
        "        # Load tokenizer with trust_remote_code\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Load model for sequence classification\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=len(train_df['label'].unique()),\n",
        "            trust_remote_code=True,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load kuleshov-group model: {e}\")\n",
        "        print(\"Trying alternative HyenaDNA model...\")\n",
        "\n",
        "        try:\n",
        "            import torch\n",
        "            from transformers import AutoConfig\n",
        "\n",
        "            model_name = \"LongSafari/hyenadna-small-32k-seqlen-hf\"  # HF-compatible version\n",
        "\n",
        "            # Create a simple character-level tokenizer for DNA\n",
        "            class DNATokenizer:\n",
        "                def __init__(self):\n",
        "                    self.vocab = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N': 4, '[PAD]': 5}\n",
        "                    self.pad_token_id = 5\n",
        "\n",
        "                def __call__(self, sequences, padding=True, truncation=True, max_length=1024, return_tensors=\"pt\"):\n",
        "                    if isinstance(sequences, str):\n",
        "                        sequences = [sequences]\n",
        "\n",
        "                    encoded = []\n",
        "                    for seq in sequences:\n",
        "                        seq = seq.upper()[:max_length] if truncation else seq.upper()\n",
        "                        tokens = [self.vocab.get(c, 4) for c in seq]  # Default to N for unknown\n",
        "\n",
        "                        if padding and len(tokens) < max_length:\n",
        "                            tokens += [self.pad_token_id] * (max_length - len(tokens))\n",
        "\n",
        "                        encoded.append(tokens)\n",
        "\n",
        "                    if return_tensors == \"pt\":\n",
        "                        import torch\n",
        "                        return {'input_ids': torch.tensor(encoded)}\n",
        "                    return {'input_ids': encoded}\n",
        "\n",
        "            tokenizer = DNATokenizer()\n",
        "\n",
        "\n",
        "            from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                model_name,\n",
        "                num_labels=len(train_df['label'].unique()),\n",
        "                trust_remote_code=True,\n",
        "                ignore_mismatched_sizes=True\n",
        "            )\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\"Could not load HyenaDNA model: {e2}\")\n",
        "            print(\"Skipping HyenaDNA for this dataset\")\n",
        "            return {\n",
        "                'dataset': dataset_name,\n",
        "                'model': 'HyenaDNA',\n",
        "                'accuracy': 0,\n",
        "                'mcc': 0,\n",
        "                'f1': 0,\n",
        "                'error': str(e2)\n",
        "            }\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Prepare sequences (HyenaDNA uses character-level)\n",
        "    print(\"Preparing sequences for HyenaDNA...\")\n",
        "    train_sequences = train_df['sequence'].str.upper().values\n",
        "    test_sequences = test_df['sequence'].str.upper().values\n",
        "    train_labels = train_df['label'].values\n",
        "    test_labels = test_df['label'].values\n",
        "\n",
        "    # Fine-tuning\n",
        "    print(\"\\nFine-tuning HyenaDNA...\")\n",
        "    model.train()\n",
        "\n",
        "    learning_rate = 1e-4\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    batch_size = 16\n",
        "    num_epochs = 1\n",
        "\n",
        "    print(f\"Training parameters:\")\n",
        "    print(f\"  - Batch size: {batch_size}\")\n",
        "    print(f\"  - Epochs: {num_epochs}\")\n",
        "    print(f\"  - Learning rate: {learning_rate}\")\n",
        "    print(f\"  - Training samples: {len(train_df)}\")\n",
        "\n",
        "    train_dataset = DNADataset(train_sequences, train_labels)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        errors = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch_sequences, batch_labels in progress_bar:\n",
        "            try:\n",
        "                # Tokenizer returns a dict with 'input_ids' key\n",
        "                inputs = tokenizer(\n",
        "                    batch_sequences.tolist() if hasattr(batch_sequences, 'tolist') else list(batch_sequences),\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=max_length,\n",
        "                    return_tensors=\"pt\"\n",
        "                )\n",
        "\n",
        "                # Move input_ids to device properly\n",
        "                input_ids = inputs['input_ids'].to(device)\n",
        "                labels = torch.tensor(batch_labels).to(device)\n",
        "\n",
        "                # Pass input_ids to model\n",
        "                outputs = model(input_ids=input_ids, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "                correct += (predictions == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                current_acc = correct / total if total > 0 else 0\n",
        "                progress_bar.set_postfix({\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'acc': f'{current_acc:.4f}',\n",
        "                    'errors': errors\n",
        "                })\n",
        "            except Exception as e:\n",
        "                errors += 1\n",
        "                # Silently continue to avoid spam\n",
        "                continue\n",
        "\n",
        "        if total > 0:\n",
        "            avg_loss = total_loss / max(1, len(train_loader) - errors)\n",
        "            final_train_acc = correct / total\n",
        "            print(f\"Epoch {epoch+1} - Average loss: {avg_loss:.4f}, Training accuracy: {final_train_acc:.4f}\")\n",
        "            if errors > 0:\n",
        "                print(f\"  (Encountered {errors} batch errors during training)\")\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    eval_errors = 0\n",
        "\n",
        "    test_dataset = DNADataset(test_sequences, test_labels)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_sequences, batch_labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            try:\n",
        "                # Tokenizer returns a dict with 'input_ids' key\n",
        "                inputs = tokenizer(\n",
        "                    batch_sequences.tolist() if hasattr(batch_sequences, 'tolist') else list(batch_sequences),\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=max_length,\n",
        "                    return_tensors=\"pt\"\n",
        "                )\n",
        "\n",
        "                # Move input_ids to device properly\n",
        "                input_ids = inputs['input_ids'].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids)\n",
        "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "                all_predictions.extend(predictions.cpu().numpy())\n",
        "                all_labels.extend(batch_labels)\n",
        "            except Exception as e:\n",
        "                eval_errors += 1\n",
        "                continue\n",
        "\n",
        "    if eval_errors > 0:\n",
        "        print(f\"  (Encountered {eval_errors} batch errors during evaluation)\")\n",
        "\n",
        "    if len(all_predictions) > 0:\n",
        "        accuracy = accuracy_score(all_labels, all_predictions)\n",
        "        mcc = matthews_corrcoef(all_labels, all_predictions)\n",
        "        f1 = f1_score(all_labels, all_predictions, average='weighted' if len(np.unique(all_labels)) > 2 else 'binary')\n",
        "\n",
        "        print(f\"\\nResults for {dataset_name}:\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"MCC: {mcc:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        cm = confusion_matrix(all_labels, all_predictions)\n",
        "        print(cm)\n",
        "    else:\n",
        "        accuracy = mcc = f1 = 0\n",
        "        print(\"No valid predictions made\")\n",
        "\n",
        "    results = {\n",
        "        'dataset': dataset_name,\n",
        "        'model': 'HyenaDNA',\n",
        "        'accuracy': float(accuracy),\n",
        "        'mcc': float(mcc),\n",
        "        'f1': float(f1),\n",
        "        'num_train_samples': len(train_df),\n",
        "        'num_test_samples': len(test_df),\n",
        "        'num_classes': len(train_df['label'].unique()),\n",
        "        'learning_rate': learning_rate,\n",
        "        'batch_size': batch_size,\n",
        "        'num_epochs': num_epochs\n",
        "    }\n",
        "\n",
        "    results_path = f\"benchmark_results/hyenadna/{dataset_name}_results.json\"\n",
        "    save_results_to_gcs(bucket_name, results_path, results)\n",
        "\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Cell X: Nucleotide Transformer benchmark function\n",
        "# ============================================\n",
        "def benchmark_nucleotide_transformer(train_df, test_df, dataset_name, bucket_name, max_length=512):\n",
        "    \"\"\"\n",
        "    Benchmark Nucleotide Transformer on a genomic dataset\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Benchmarking Nucleotide Transformer on {dataset_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    print(\"Loading Nucleotide Transformer model...\")\n",
        "\n",
        "    # Use the 500M human reference model (best for human genomic tasks)\n",
        "    model_name = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
        "    # Alternative options:\n",
        "    # \"InstaDeepAI/nucleotide-transformer-500m-1000g\" - trained on 1000 genomes\n",
        "    # \"InstaDeepAI/nucleotide-transformer-2.5b-1000g\" - larger but slower\n",
        "\n",
        "    try:\n",
        "        # Load tokenizer with trust_remote_code\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Determine number of classes\n",
        "        num_classes = len(train_df['label'].unique())\n",
        "        print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "        # Load model for sequence classification\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=num_classes,\n",
        "            trust_remote_code=True,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Nucleotide Transformer: {e}\")\n",
        "        print(\"Skipping Nucleotide Transformer for this dataset\")\n",
        "        return {\n",
        "            'dataset': dataset_name,\n",
        "            'model': 'NucleotideTransformer',\n",
        "            'accuracy': 0,\n",
        "            'mcc': 0,\n",
        "            'f1': 0,\n",
        "            'error': str(e)\n",
        "        }\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Prepare sequences (Nucleotide Transformer uses character-level tokenization)\n",
        "    print(\"Preparing sequences for Nucleotide Transformer...\")\n",
        "    train_sequences = train_df['sequence'].str.upper().values\n",
        "    test_sequences = test_df['sequence'].str.upper().values\n",
        "    train_labels = train_df['label'].values\n",
        "    test_labels = test_df['label'].values\n",
        "\n",
        "    # Fine-tuning\n",
        "    print(\"\\nFine-tuning Nucleotide Transformer...\")\n",
        "    model.train()\n",
        "\n",
        "    # Use same learning rate as other models for consistency\n",
        "    learning_rate = 3e-5\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    batch_size = 8  # Smaller batch size as NT is larger\n",
        "    num_epochs = 1\n",
        "\n",
        "    print(f\"Training parameters:\")\n",
        "    print(f\"  - Batch size: {batch_size}\")\n",
        "    print(f\"  - Epochs: {num_epochs}\")\n",
        "    print(f\"  - Learning rate: {learning_rate}\")\n",
        "    print(f\"  - Training samples: {len(train_df)}\")\n",
        "\n",
        "    train_dataset = DNADataset(train_sequences, train_labels)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch_sequences, batch_labels in progress_bar:\n",
        "            try:\n",
        "                # Tokenize batch\n",
        "                inputs = tokenizer(\n",
        "                    list(batch_sequences),\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=max_length,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).to(device)\n",
        "\n",
        "                labels = torch.tensor(batch_labels).to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(**inputs, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "                correct += (predictions == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Backward pass\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                current_acc = correct / total if total > 0 else 0\n",
        "                progress_bar.set_postfix({\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'acc': f'{current_acc:.4f}'\n",
        "                })\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                if \"out of memory\" in str(e):\n",
        "                    print(f\"WARNING: Out of memory, skipping batch\")\n",
        "                    if hasattr(torch.cuda, 'empty_cache'):\n",
        "                        torch.cuda.empty_cache()\n",
        "                    continue\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "        if total > 0:\n",
        "            avg_loss = total_loss / len(train_loader)\n",
        "            final_train_acc = correct / total\n",
        "            print(f\"Epoch {epoch+1} - Average loss: {avg_loss:.4f}, Training accuracy: {final_train_acc:.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    test_dataset = DNADataset(test_sequences, test_labels)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # Larger batch for eval\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_sequences, batch_labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            try:\n",
        "                inputs = tokenizer(\n",
        "                    list(batch_sequences),\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=max_length,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).to(device)\n",
        "\n",
        "                outputs = model(**inputs)\n",
        "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "                all_predictions.extend(predictions.cpu().numpy())\n",
        "                all_labels.extend(batch_labels)\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                if \"out of memory\" in str(e):\n",
        "                    print(f\"WARNING: Out of memory during eval, skipping batch\")\n",
        "                    if hasattr(torch.cuda, 'empty_cache'):\n",
        "                        torch.cuda.empty_cache()\n",
        "                    continue\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "    if len(all_predictions) > 0:\n",
        "        accuracy = accuracy_score(all_labels, all_predictions)\n",
        "        mcc = matthews_corrcoef(all_labels, all_predictions)\n",
        "        f1 = f1_score(all_labels, all_predictions, average='weighted' if num_classes > 2 else 'binary')\n",
        "\n",
        "        print(f\"\\nResults for {dataset_name}:\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"MCC: {mcc:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        cm = confusion_matrix(all_labels, all_predictions)\n",
        "        print(cm)\n",
        "    else:\n",
        "        accuracy = mcc = f1 = 0\n",
        "        print(\"No valid predictions made\")\n",
        "\n",
        "    results = {\n",
        "        'dataset': dataset_name,\n",
        "        'model': 'NucleotideTransformer',\n",
        "        'accuracy': float(accuracy),\n",
        "        'mcc': float(mcc),\n",
        "        'f1': float(f1),\n",
        "        'num_train_samples': len(train_df),\n",
        "        'num_test_samples': len(test_df),\n",
        "        'num_classes': num_classes,\n",
        "        'learning_rate': learning_rate,\n",
        "        'batch_size': batch_size,\n",
        "        'num_epochs': num_epochs\n",
        "    }\n",
        "\n",
        "    # Save to GCS\n",
        "    results_path = f\"benchmark_results/nucleotide_transformer/{dataset_name}_results.json\"\n",
        "    save_results_to_gcs(bucket_name, results_path, results)\n",
        "\n",
        "    # Clear GPU memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "bf1SxOB5eh0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ajbm_mcFeh8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3uK9tOC0cyt"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 8: Main benchmarking pipeline\n",
        "# ============================================\n",
        "def run_benchmarks(bucket_name, datasets, models_to_run=['dnabert', 'hyenadna']):\n",
        "    \"\"\"\n",
        "    Run benchmarks on multiple datasets\n",
        "    \"\"\"\n",
        "    all_results = []\n",
        "\n",
        "    for dataset_name in datasets:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing dataset: {dataset_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Load data from GCS\n",
        "        train_path = f\"genomic_benchmarks/{dataset_name}_train.csv\"\n",
        "        test_path = f\"genomic_benchmarks/{dataset_name}_test.csv\"\n",
        "\n",
        "        print(f\"Loading data from GCS...\")\n",
        "        train_df = load_csv_from_gcs(bucket_name, train_path)\n",
        "        test_df = load_csv_from_gcs(bucket_name, test_path)\n",
        "\n",
        "        print(f\"Train samples: {len(train_df)}\")\n",
        "        print(f\"Test samples: {len(test_df)}\")\n",
        "        print(f\"Sequence length: {len(train_df.iloc[0]['sequence'])}\")\n",
        "\n",
        "        # Run DNABERT\n",
        "        if 'dnabert' in models_to_run:\n",
        "            try:\n",
        "                dnabert_results = benchmark_dnabert(train_df, test_df, dataset_name, bucket_name)\n",
        "                all_results.append(dnabert_results)\n",
        "            except Exception as e:\n",
        "                print(f\"Error running DNABERT on {dataset_name}: {e}\")\n",
        "\n",
        "        # Run HyenaDNA\n",
        "        if 'hyenadna' in models_to_run:\n",
        "            try:\n",
        "                hyena_results = benchmark_hyena_dna(train_df, test_df, dataset_name, bucket_name)\n",
        "                all_results.append(hyena_results)\n",
        "            except Exception as e:\n",
        "                print(f\"Error running HyenaDNA on {dataset_name}: {e}\")\n",
        "\n",
        "        # Run Nucleotide Transformer\n",
        "        if 'nucleotide_transformer' in models_to_run:\n",
        "            try:\n",
        "                nt_results = benchmark_nucleotide_transformer(train_df, test_df, dataset_name, bucket_name)\n",
        "                all_results.append(nt_results)\n",
        "            except Exception as e:\n",
        "                print(f\"Error running Nucleotide Transformer on {dataset_name}: {e}\")\n",
        "\n",
        "    # Save all results\n",
        "    all_results_path = \"benchmark_results/all_results.json\"\n",
        "    save_results_to_gcs(bucket_name, all_results_path, all_results)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BENCHMARK SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    print(results_df.to_string())\n",
        "\n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Apxf89FF0c10"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 9: Run the benchmarks!\n",
        "# ============================================\n",
        "# Configuration\n",
        "BUCKET_NAME = \"YOUR_BUCKET\"  # Your GCS bucket\n",
        "\n",
        "# List of datasets to benchmark\n",
        "DATASETS = [\n",
        "    \"human_ocr_ensembl\",\n",
        "    \"demo_coding_vs_intergenomic_seqs\",\n",
        "    \"demo_human_or_worm\",\n",
        "    \"human_enhancers_cohn\",\n",
        "    \"human_enhancers_ensembl\",\n",
        "    \"human_ensembl_regulatory\",\n",
        "    \"human_nontata_promoters\",\n",
        "    \"human_ocr_ensembl\"\n",
        "]\n",
        "\n",
        "# Which models to run\n",
        "MODELS = ['dnabert', 'hyenadna', 'nucleotide_transformer']\n",
        "\n",
        "# Run benchmarks\n",
        "results_df = run_benchmarks(BUCKET_NAME, DATASETS, MODELS)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}