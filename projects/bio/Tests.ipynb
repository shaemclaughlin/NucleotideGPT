{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42292b5e-9efd-4d7d-b67e-82873e50e4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 22:10:27.051258: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-03 22:10:27.059447: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-03 22:10:27.167545: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-03 22:10:27.209176: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-03 22:10:27.222129: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-03 22:10:27.327612: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-03 22:10:28.986723: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "import io\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04b7d335-5ea7-40d0-9425-a8c7bf659ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING = \"P\"\n",
    "VOCAB = [PADDING, \"U\", \"A\", \"C\", \"G\", \"T\", \"N\"]\n",
    "DNA_TOKENS = {ch: i for i, ch in enumerate(VOCAB)}\n",
    "# This makes:\n",
    "# PADDING = 0\n",
    "# U = 1 \n",
    "# A = 2\n",
    "# C = 3\n",
    "# G = 4\n",
    "# T = 5\n",
    "# N = 6\n",
    "\n",
    "def tokenize_dna(sequence):\n",
    "    return [DNA_TOKENS.get(base, 1) for base in sequence.upper()]  # Unknown bases map to U(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b103e112-e0fb-4305-ade5-1635e3cb82fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sequence: ACGT\n",
      "Tokens: [2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_seq = \"ACGT\"\n",
    "tokens = tokenize_dna(test_seq)\n",
    "print(f\"Test sequence: {test_seq}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "assert tokens == [2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6642dc8b-141f-4a17-8d18-7d08dc91b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_to_tfrecord(species_name, max_padding=100):\n",
    "   \"\"\"Process CSV to TFRecord with tests and validation\"\"\"\n",
    "   \n",
    "   # Load CSV\n",
    "   bucket_name = \"minformer_data\"\n",
    "   storage_client = storage.Client()\n",
    "   bucket = storage_client.get_bucket(bucket_name)\n",
    "   csv_blob = bucket.blob(f\"eukaryote_pands/{species_name}.csv\")\n",
    "   content = csv_blob.download_as_string()\n",
    "   df = pd.read_csv(io.BytesIO(content))\n",
    "   \n",
    "   print(f\"Processing {species_name}\")\n",
    "   print(f\"Total sequences: {len(df)}\")\n",
    "   \n",
    "   valid_sequences = []\n",
    "   skipped_padding = 0\n",
    "   skipped_n = 0\n",
    "   \n",
    "   # Validate sequences\n",
    "   for _, row in tqdm(df.iterrows()):\n",
    "       seq = row[\"Sequence\"]\n",
    "       \n",
    "       # Skip if starts with N\n",
    "       if seq[0:5] == \"NNNNN\":\n",
    "           skipped_n += 1\n",
    "           continue\n",
    "           \n",
    "       # Check padding needed\n",
    "       padding_needed = 8192 - len(seq)\n",
    "       if padding_needed > max_padding:\n",
    "           skipped_padding += 1\n",
    "           continue\n",
    "           \n",
    "       valid_sequences.append(row)\n",
    "       \n",
    "   print(f\"Sequences after filtering:\")\n",
    "   print(f\"Skipped due to N's: {skipped_n}\")\n",
    "   print(f\"Skipped due to padding: {skipped_padding}\")\n",
    "   print(f\"Valid sequences: {len(valid_sequences)}\")\n",
    "   \n",
    "   # Save valid sequences in batches\n",
    "   batch_size = 128\n",
    "   for i in range(0, len(valid_sequences), batch_size):\n",
    "       batch = valid_sequences[i:i+batch_size]\n",
    "       output_file = f\"gs://minformer_data/diverse_genomes_tf/{species_name}/tfrecords/record_{i//batch_size}.tfrecord\"\n",
    "       \n",
    "       with tf.io.TFRecordWriter(output_file) as writer:\n",
    "           for row in batch:\n",
    "               # Tokenize and pad\n",
    "               tokens = tokenize_dna(row[\"Sequence\"])\n",
    "               padding_needed = 8192 - len(tokens)\n",
    "               tokens = np.pad(tokens, (0, padding_needed))  # Pad end only\n",
    "               segment_ids = np.ones(8192)\n",
    "               \n",
    "               # Create TF Example\n",
    "               example = tf.train.Example(\n",
    "                   features=tf.train.Features(\n",
    "                       feature={\n",
    "                           \"x\": tf.train.Feature(int64_list=tf.train.Int64List(value=tokens)),\n",
    "                           \"segment_ids\": tf.train.Feature(int64_list=tf.train.Int64List(value=segment_ids))\n",
    "                       }\n",
    "                   )\n",
    "               )\n",
    "               writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5917811a-8580-4a92-91eb-314d20776710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing human_genome_8192bp_bins_no_N\n",
      "Total sequences: 357933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "357933it [00:36, 9923.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences after filtering:\n",
      "Skipped due to N's: 0\n",
      "Skipped due to padding: 0\n",
      "Valid sequences: 357933\n"
     ]
    }
   ],
   "source": [
    "# Test processing\n",
    "test_species = \"human_genome_8192bp_bins_no_N\"\n",
    "process_csv_to_tfrecord(test_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b425a2ce-b03e-4855-952b-a76011266642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting Bradyrhizobium_japonicum_8192bp_bins_no_N:\n",
      "\n",
      "Sequence 1:\n",
      "First 50 tokens: [4, 4, 1, 1, 4, 4, 1, 1, 4, 1, 3, 4, 2, 4, 4, 4, 3, 1, 2, 4, 3, 2, 1, 1, 4, 1, 2, 4, 3, 3, 3, 2, 3, 1, 4, 1, 4, 3, 1, 4, 2, 2, 3, 3, 1, 1, 3, 2, 3, 2]\n",
      "First 50 segment_ids: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Token counts: {1: 1621, 2: 2386, 3: 2561, 4: 1624}\n",
      "Decoded sequence: TTAATTAATAGTCTTTGACTGCAATACTGGGCGATATGATCCGGAAGCGC...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# These should match exactly what's in diverse_genomes_tfs.py\n",
    "PADDING = \"P\"\n",
    "VOCAB = [PADDING, \"A\", \"C\", \"G\", \"T\", \"N\"]\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "stoi = {ch: i for i, ch in enumerate(VOCAB)}\n",
    "itos = {i: ch for i, ch in enumerate(VOCAB)}\n",
    "\n",
    "def inspect_first_species():\n",
    "    species = 'Bradyrhizobium_japonicum_8192bp_bins_no_N'\n",
    "    file_pattern = f\"gs://minformer_data/diverse_genomes_tf_v2/{species}/tfrecords/record_0.tfrecord\"\n",
    "    \n",
    "    try:\n",
    "        dataset = tf.data.TFRecordDataset([file_pattern])\n",
    "        record = next(iter(dataset))\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(record.numpy())\n",
    "        tokens = list(example.features.feature['x'].int64_list.value)\n",
    "        segment_ids = list(example.features.feature['segment_ids'].int64_list.value)\n",
    "        \n",
    "        print(f\"\\nInspecting {species}:\")\n",
    "        print(\"\\nSequence 1:\")\n",
    "        print(\"First 50 tokens:\", tokens[:50])\n",
    "        print(\"First 50 segment_ids:\", segment_ids[:50])\n",
    "        token_counts = dict(zip(*np.unique(tokens, return_counts=True)))\n",
    "        print(\"Token counts:\", token_counts)\n",
    "        \n",
    "        # Use our tokenization scheme to verify\n",
    "        decoded_seq = ''.join(itos[t] for t in tokens[:50])\n",
    "        print(f\"Decoded sequence: {decoded_seq}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "\n",
    "inspect_first_species()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
